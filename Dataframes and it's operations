DATAFRAME ANS IT'S OPERATIONS:
=============================

DIFFERENCE BETWEEN RDD AND DATAFRAME:
------------------------------------
RDD
The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.

RDD Features:-
Distributed collection:
RDD uses MapReduce operations which is widely adopted for processing and generating large datasets with a parallel, distributed algorithm on a cluster. It allows users to write parallel computations, using a set of high-level operators, without having to worry about work distribution and fault tolerance.

Immutable: RDDs composed of a collection of records which are partitioned. A partition is a basic unit of parallelism in an RDD, and each partition is one logical division of data which is immutable and created through some transformations on existing partitions.Immutability helps to achieve consistency in computations.

Fault tolerant: In a case of we lose some partition of RDD , we can replay the transformation on that partition in lineage to achieve the same computation, rather than doing data replication across multiple nodes.This characteristic is the biggest benefit of RDD because it saves a lot of efforts in data management and replication and thus achieves faster computations.

Lazy evaluations: All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset . The transformations are only computed when an action requires a result to be returned to the driver program.

Functional transformations: RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.

Data processing formats:
It can easily and efficiently process data which is structured as well as unstructured data.

Programming Languages supported:
RDD API is available in Java, Scala, Python and R.

RDD Limitations:-
No inbuilt optimization engine: When working with structured data, RDDs cannot take advantages of Spark’s advanced optimizers including catalyst optimizer and Tungsten execution engine. Developers need to optimize each RDD based on its attributes.

Handling structured data: Unlike Dataframe and datasets, RDDs don’t infer the schema of the ingested data and requires the user to specify it.

Dataframes
Spark introduced Dataframes in Spark 1.3 release. Dataframe overcomes the key challenges that RDDs had.

A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a R/Python Dataframe. Along with Dataframe, Spark also introduced catalyst optimizer, which leverages advanced programming features to build an extensible query optimizer.

Dataframe Features:-
Distributed collection of Row Object: A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database, but with richer optimizations under the hood.

Data Processing: Processing structured and unstructured data formats (Avro, CSV, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, MySQL, etc). It can read and write from all these various datasources.

Optimization using catalyst optimizer: It powers both SQL queries and the DataFrame API. Dataframe use catalyst tree transformation framework in four phases,

1.Analyzing a logical plan to resolve references
2.Logical plan optimization
3.Physical planning
4.Code generation to compile parts of the query to Java bytecode.
Hive Compatibility: Using Spark SQL, you can run unmodified Hive queries on your existing Hive warehouses. It reuses Hive frontend and MetaStore and gives you full compatibility with existing Hive data, queries, and UDFs.

Tungsten: Tungsten provides a physical execution backend whichexplicitly manages memory and dynamically generates bytecode for expression evaluation.

Programming Languages supported:
Dataframe API is available in Java, Scala, Python, and R.

Dataframe Limitations:-
Compile-time type safety: As discussed, Dataframe API does not support compile time safety which limits you from manipulating data when the structure is not know. The following example works during compile time. However, you will get a Runtime exception when executing this code.
Example:

case class Person(name : String , age : Int) 
val dataframe = sqlContext.read.json("people.json") 
dataframe.filter("salary > 10000").show 
=> throws Exception : cannot resolve 'salary' given input age , name
This is challenging specially when you are working with several transformation and aggregation steps.

Cannot operate on domain Object (lost domain object): Once you have transformed a domain object into dataframe, you cannot regenerate it from it. In the following example, once we have create personDF from personRDD, we won’t be recover the original RDD of Person class (RDD[Person]).
Example:

case class Person(name : String , age : Int)
val personRDD = sc.makeRDD(Seq(Person("A",10),Person("B",20)))
val personDF = sqlContext.createDataframe(personRDD)
personDF.rdd // returns RDD[Row] , does not returns RDD[Person]
Datasets API
Dataset API is an extension to DataFrames that provides a type-safe, object-oriented programming interface. It is a strongly-typed, immutable collection of objects that are mapped to a relational schema.

At the core of the Dataset, API is a new concept called an encoder, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization. Spark 1.6 comes with support for automatically generating encoders for a wide variety of types, including primitive types (e.g. String, Integer, Long), Scala case classes, and Java Beans.

Dataset Features:-
Provides best of both RDD and Dataframe: RDD(functional programming, type safe), DataFrame (relational model, Query optimazation , Tungsten execution, sorting and shuffling)

Encoders: With the use of Encoders, it is easy to convert any JVM object into a Dataset, allowing users to work with both structured and unstructured data unlike Dataframe.

Programming Languages supported: Datasets API is currently only available in Scala and Java. Python and R are currently not supported in version 1.6. Python support is slated for version 2.0.

Type Safety: Datasets API provides compile time safety which was not available in Dataframes. In the example below, we can see how Dataset can operate on domain objects with compile lambda functions.

Example:

case class Person(name : String , age : Int)
val personRDD = sc.makeRDD(Seq(Person("A",10),Person("B",20)))
val personDF = sqlContext.createDataframe(personRDD)
val ds:Dataset[Person] = personDF.as[Person]
ds.filter(p => p.age > 25)
ds.filter(p => p.salary > 25)
 // error : value salary is not a member of person
ds.rdd // returns RDD[Person]
Interoperable: Datasets allows you to easily convert your existing RDDs and Dataframes into datasets without boilerplate code.
Datasets API Limitation:-
Requires type casting to String: Querying the data from datasets currently requires us to specify the fields in the class as a string. Once we have queried the data, we are forced to cast column to the required data type. On the other hand, if we use map operation on Datasets, it will not use Catalyst optimizer.
Example:

ds.select(col("name").as[String], $"age".as[Int]).collect()
No support for Python and R: As of release 1.6, Datasets only support Scala and Java. Python support will be introduced in Spark 2.0.

The Datasets API brings in several advantages over the existing RDD and Dataframe API with better type safety and functional programming.With the challenge of type casting requirements in the API, you would still not the required type safety and will make your code brittle.

============================================================================================================================================

In RDD if we load data and to extract some information we need to define map and split function.Because there is no schema defined to it.

For example:

val data = sc.textFile("/user/selfcoursebigdata2042/girish/Sqoop/retail_data/orders")
scala> data.take(10).foreach(println)

-1,2018-09-10 00:00:00.0,3017,COMPLETE
1,2013-07-25 00:00:00.0,11599,CLOSED
2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT
3,2013-07-25 00:00:00.0,12111,COMPLETE
4,2013-07-25 00:00:00.0,8827,CLOSED
5,2013-07-25 00:00:00.0,11318,COMPLETE
6,2013-07-25 00:00:00.0,7130,COMPLETE
7,2013-07-25 00:00:00.0,4530,COMPLETE
8,2013-07-25 00:00:00.0,2911,PROCESSING
9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT


To extract date, we need to write map function as below

scala> data.map(rec => rec.split(",")(1)).take(10)foreach(println)
2018-09-10 00:00:00.0
2013-07-25 00:00:00.0
2013-07-25 00:00:00.0
2013-07-25 00:00:00.0	
2013-07-25 00:00:00.0
2013-07-25 00:00:00.0
2013-07-25 00:00:00.0
2013-07-25 00:00:00.0
2013-07-25 00:00:00.0
2013-07-25 00:00:00.0

But there is no column defined

Now let's consider the Dataframne, as per it's definiton we will have a data along with it's schema defined.Lets take the example for creating dataframe with json data.

Before that there is an api in spark which tells what are the file formats that are supported by it:

scala> spark.read.
csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile

We read the data from variuos file formats specifed above.

scala> val orders = spark.read.json("/user/selfcoursebigdata2042/girish/Datasets/retail_db_json/orders")
orders: org.apache.spark.sql.DataFrame = [order_customer_id: bigint, order_date: string ... 2 more fields]

scala> orders.printSchema
root
 |-- order_customer_id: long (nullable = true)
 |-- order_date: string (nullable = true)
 |-- order_id: long (nullable = true)
 |-- order_status: string (nullable = true)

 As per the data in json format, each value in the field is tied to it's name like

 {"order_id":1,"order_date":"2013-07-25 00:00:00.0","order_customer_id":11599,"order_status":"CLOSED"}
{"order_id":2,"order_date":"2013-07-25 00:00:00.0","order_customer_id":256,"order_status":"PENDING_PAYMENT"}
{"order_id":3,"order_date":"2013-07-25 00:00:00.0","order_customer_id":12111,"order_status":"COMPLETE"}
{"order_id":4,"order_date":"2013-07-25 00:00:00.0","order_customer_id":8827,"order_status":"CLOSED"}

So there is a specified column name for each json field along with it's value which was sepecrated by ":".

scala> orders.show
+-----------------+--------------------+--------+---------------+
|order_customer_id|          order_date|order_id|   order_status|
+-----------------+--------------------+--------+---------------+
|            11599|2013-07-25 00:00:...|       1|         CLOSED|
|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|
|            12111|2013-07-25 00:00:...|       3|       COMPLETE|
|             8827|2013-07-25 00:00:...|       4|         CLOSED|
|            11318|2013-07-25 00:00:...|       5|       COMPLETE|
|             7130|2013-07-25 00:00:...|       6|       COMPLETE|
|             4530|2013-07-25 00:00:...|       7|       COMPLETE|
|             2911|2013-07-25 00:00:...|       8|     PROCESSING|
|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|
|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|
|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|
|             1837|2013-07-25 00:00:...|      12|         CLOSED|
|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|
|             9842|2013-07-25 00:00:...|      14|     PROCESSING|
|             2568|2013-07-25 00:00:...|      15|       COMPLETE|
|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|
|             2667|2013-07-25 00:00:...|      17|       COMPLETE|
|             1205|2013-07-25 00:00:...|      18|         CLOSED|
|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|
|             9198|2013-07-25 00:00:...|      20|     PROCESSING|
+-----------------+--------------------+--------+---------------+
only showing top 20 rows


But when we deal with other dat formats like csv the case will be 

scala> val orders = spark.read.csv("/user/selfcoursebigdata2042/girish/Datasets/retail_db/orders")
orders: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 2 more fields]

scala> orders.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)

It creates the dataframe with some orbitary column name, but we can use toDF to specify column names.

scala> val orders = spark.read.csv("/user/selfcoursebigdata2042/girish/Datasets/retail_db/orders").toDF("order_customer_id","order_date","order_id","order_status")
orders: org.apache.spark.sql.DataFrame = [order_customer_id: string, order_date: string ... 2 more fields]

scala> orders.printSchema
root
 |-- order_customer_id: string (nullable = true)
 |-- order_date: string (nullable = true)
 |-- order_id: string (nullable = true)
 |-- order_status: string (nullable = true)

Even though it changes the column name but it doesnot change the type.

To change the type, there is an effective way 

scala> val orders = sc.textFile("/user/selfcoursebigdata2042/girish/Datasets/retail_db/orders").
     | map(rec => (rec.split(",")(0).toInt,rec.split(",")(1),rec.split(",")(2).toInt,rec.split(",")(3)))
     .toDF("order_customer_id","order_date","order_id","order_status")
orders: org.apache.spark.sql.DataFrame = [order_customer_id: int, order_date: string ... 2 more fields]

scala> orders.printSchema
root
 |-- order_customer_id: integer (nullable = false)
 |-- order_date: string (nullable = true)
 |-- order_id: integer (nullable = false)
 |-- order_status: string (nullable = true)


DATASET
=======

We can create dataset using toDS like the way we created DF but there are some limitations.We cannot use the tuples like we have used in map transformation.

map(rec => (rec.split(",")(0).toInt,rec.split(",")(1),rec.split(",")(2).toInt,rec.split(",")(3)))

We can create dataset either from RDD or D. We have to use Case Class

Ex: case class Orders(order_id : Int,order_date : String,order_customer_id : Int,order_status : String )

It gives something like Boiler plate code which means it gives certains functionality to the class and we can access all the variables using 

scala> :javap -p Orders
Compiled from "<console>"
public class Orders implements scala.Product,scala.Serializable {
  private final int order_id;
  private final java.lang.String order_date;
  private final int order_customer_id;
  private final java.lang.String order_status;
  public final $iw $outer;
  public int order_id();
  public java.lang.String order_date();
  public int order_customer_id();
  public java.lang.String order_status();
  public Orders copy(int, java.lang.String, int, java.lang.String);
  public int copy$default$1();
  public java.lang.String copy$default$2();
  public int copy$default$3();
  public java.lang.String copy$default$4();
  public java.lang.String productPrefix();
  public int productArity();
  public java.lang.Object productElement(int);
  public scala.collection.Iterator<java.lang.Object> productIterator();
  public boolean canEqual(java.lang.Object);
  public int hashCode();
  public java.lang.String toString();
  public boolean equals(java.lang.Object);
  public $iw $line14$$read$Orders$$$outer();
  public Orders($iw, int, java.lang.String, int, java.lang.String);
}


It contains the two interfaces namely Product and Serializable.

Srerializable:
The concept of serailizable is by default whenever we instatiate/create an object using a java class, the scope of the java object is pertain to that JVM only with in the memory. It means  the object are flushed out of memory once the session expires which he program exit.

Using Seriablize we can pass the JVM to other node and make availabe for other class or we can save byte of an class. By default case class is Serailizable.

Product:
We can use gthe functions like productPrefix,productElement.

And by default all the variables in case class are immutalbe and they are just like final varaibles in java.

CREATION OF DATASET FROM RDD:
============================
CODE:
====
case class Order(order_id : Int,order_date : String,order_customer_id : Int,order_status : String )
val orders = sc.textFile("/user/selfcoursebigdata2042/girish/Datasets/retail_db/orders").
map(o => {
 val a = o.split(",")
 Order(a(0).toInt,a(1),a(2).toInt,a(3))
 }).toDS

RESULT:
======
 scala> case class Order(order_id : Int,order_date : String,order_customer_id : Int,order_status : String )
defined class Order

scala> val orders = sc.textFile("/user/selfcoursebigdata2042/girish/Datasets/retail_db/orders").
     | map(o => {
     |  val a = o.split(",")
     |  Order(a(0).toInt,a(1),a(2).toInt,a(3))
     |  }).toDS
orders: org.apache.spark.sql.Dataset[Order] = [order_id: int, order_date: string ... 2 more fields]

CREATION OF DATASET FROM DATAFRAME:
==================================
case class Order(order_id : Long,order_date : String,order_customer_id : Long,order_status : String )
val orderDF = spark.read.json("/user/selfcoursebigdata2042/girish/Datasets/retail_db_json/orders"").as[Order]

scala> case class Order(order_id : Long,order_date : String,order_customer_id : Long,order_status : String )
defined class Order

scala> val orderDS = spark.read.json("/user/selfcoursebigdata2042/girish/Datasets/retail_db_json/orders").as[Order]
orderDF: org.apache.spark.sql.Dataset[Order] = [order_customer_id: bigint, order_date: string ... 2 more fields]

scala> orderDS.printSchema
root
 |-- order_customer_id: long (nullable = true)
 |-- order_date: string (nullable = true)
 |-- order_id: long (nullable = true)
 |-- order_status: string (nullable = true)

scala> orderDS.show
+-----------------+--------------------+--------+---------------+
|order_customer_id|          order_date|order_id|   order_status|
+-----------------+--------------------+--------+---------------+
|            11599|2013-07-25 00:00:...|       1|         CLOSED|
|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|
|            12111|2013-07-25 00:00:...|       3|       COMPLETE|
|             8827|2013-07-25 00:00:...|       4|         CLOSED|
|            11318|2013-07-25 00:00:...|       5|       COMPLETE|
|             7130|2013-07-25 00:00:...|       6|       COMPLETE|
|             4530|2013-07-25 00:00:...|       7|       COMPLETE|
|             2911|2013-07-25 00:00:...|       8|     PROCESSING|
|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|
|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|
|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|
|             1837|2013-07-25 00:00:...|      12|         CLOSED|
|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|
|             9842|2013-07-25 00:00:...|      14|     PROCESSING|
|             2568|2013-07-25 00:00:...|      15|       COMPLETE|
|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|
|             2667|2013-07-25 00:00:...|      17|       COMPLETE|
|             1205|2013-07-25 00:00:...|      18|         CLOSED|
|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|
|             9198|2013-07-25 00:00:...|      20|     PROCESSING|
+-----------------+--------------------+--------+---------------+
only showing top 20 rows


HOW TO REGISTER PACKAGES WITH SPARK:
===================================

We have to launch the spark-shell using the following command and by giving the compatiable avro version incompaltiable with spark version from the following link:

https://github.com/databricks/spark-avro

$ bin/spark-shell --packages com.databricks:spark-avro_2.11:4.0.0

WRITE THE DATA USING SPARK:
==========================

val ordersJson = spark.read.json("/user/selfcoursebigdata2042/girish/Datasets/retail_db_json/orders")
// Creates a DF whose name is ordersJson

scala> val ordersJson = spark.read.json("/user/selfcoursebigdata2042/girish/Datasets/retail_db_json/orders")
ordersJson: org.apache.spark.sql.DataFrame = [order_customer_id: bigint, order_date: string ... 2 more fields]

scala> ordersJson.printSchema
root
 |-- order_customer_id: long (nullable = true)
 |-- order_date: string (nullable = true)
 |-- order_id: long (nullable = true)
 |-- order_status: string (nullable = true)


scala> ordersJson.show(10)
+-----------------+--------------------+--------+---------------+
|order_customer_id|          order_date|order_id|   order_status|
+-----------------+--------------------+--------+---------------+
|            11599|2013-07-25 00:00:...|       1|         CLOSED|
|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|
|            12111|2013-07-25 00:00:...|       3|       COMPLETE|
|             8827|2013-07-25 00:00:...|       4|         CLOSED|
|            11318|2013-07-25 00:00:...|       5|       COMPLETE|
|             7130|2013-07-25 00:00:...|       6|       COMPLETE|
|             4530|2013-07-25 00:00:...|       7|       COMPLETE|
|             2911|2013-07-25 00:00:...|       8|     PROCESSING|
|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|
|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|
+-----------------+--------------------+--------+---------------+
only showing top 10 rows

After registering the avro package, to use it we need to import the package

scala> import com.databricks.spark.avro._
import com.databricks.spark.avro._
// Importing avro package to use it in our program

scala> ordersJson.write.
avro   bucketBy   csv   format   insertInto   jdbc   json   mode   option   options   orc   parquet   partitionBy   save   saveAsTable   sortBy   text

scala> ordersJason.write.avro("/user/selfcoursebigdata2042/girish/Datasets/retail_db_avro/orders")
// Wrting data using Dataframe API in Spark in avro file format

// Reading the avro data from Spark Dataframe API

scala> spark.read.avro("/user/selfcoursebigdata2042/girish/Datasets/retail_db_avro/orders")
res3: org.apache.spark.sql.DataFrame = [order_customer_id: bigint, order_date: string ... 2 more fields]

scala> spark.read.avro("/user/selfcoursebigdata2042/girish/Datasets/retail_db_avro/orders").show(10)
+-----------------+--------------------+--------+---------------+
|order_customer_id|          order_date|order_id|   order_status|
+-----------------+--------------------+--------+---------------+
|            11599|2013-07-25 00:00:...|       1|         CLOSED|
|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|
|            12111|2013-07-25 00:00:...|       3|       COMPLETE|
|             8827|2013-07-25 00:00:...|       4|         CLOSED|
|            11318|2013-07-25 00:00:...|       5|       COMPLETE|
|             7130|2013-07-25 00:00:...|       6|       COMPLETE|
|             4530|2013-07-25 00:00:...|       7|       COMPLETE|
|             2911|2013-07-25 00:00:...|       8|     PROCESSING|
|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|
|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|
+-----------------+--------------------+--------+---------------+
only showing top 10 rows

We can use the select statement using Dataframes to get results

scala> ordersJson.select("order_id","order_date").show

In order to apply any fuctions like lower(),upper() etc on any column we need to apply column type along with all the column in the select statement.

// col type and it's importance
scala> ordersJson.select(col("order_id"),col(lower("order_status"))).show

The shorthand notation of col is $

scala> ordersJson.select($"order_id",lower($"order_status")).show

// alias function 
scala> ordersJson.select(col("order_id"),col(lower("order_status")).alias(Orders_Status)).show

