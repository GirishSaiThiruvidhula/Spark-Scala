Spark 2.0 vs Spark 2.3.0
========================

val orders = spark.read.json("/user/selfcoursebigdata2042/girish/Sqoop/retail_data/orders")

val orderItems = spark.read.json("/user/selfcoursebigdata2042/girish/Sqoop/retail_data/order_Items")

orders.printSchema

It contains 4 columns 

-> order_customer_id,
   order_date,
   order_id, 
   order_status

orderItems.printSchema

It contains 6 columns -

order_item_id
order_item_order_id
order_item_product_id
order_item_product_price
order_item_quantity
order_item_subtotal


To process the data we will be doing the following things:

Selection or Projection of data(Row level transformations -> Data Masking,standardizing the address, Data cleansing in some fields)

Aggregations( Total or by key transformations)

Sorting( total or by key sorting)

Ranking( total or key ranking)

Joins

Structued Data - Spark SQL

Semi structured ,GraphX and ML- Dataframes

Function validation(Implementing the function and verifying it's output)
====================

In Oracle, inorder to validate a function, we use a able called dual  which contains 1 col and 1 row and we can apply functions on top 
of it.

//select * from dual;

if we are using Mysql, we can pass the values and verify the output like mentioned below:

select substring("Hello word", 2, 5)

While coming to Dataframe, you need DF to perform validations

// Stimulating Oracle dual funtion in Dataframes

val dual = Seq("ABC").toDF("dummy")

In Oracle to get System date we do:

Ex: Select sysdate from dual 

Below is the package that gives the access to all the functions that are available in Spark:

org.apache.spark.sql.functions. and we need to press TAB to get the list of all the functions:

The DUAL table is a special one-row, one-column table present by default in Oracle and other database installations. 
In Oracle, the table has a single VARCHAR2(1) column called DUMMY that has a value of 'X'. 
It is suitable for use in selecting a pseudo column such as SYSDATE or USER.


 
